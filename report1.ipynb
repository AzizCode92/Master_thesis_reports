{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center>  First report of my master thesis  (Layer normalization ) </center> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During my first month (Ocotber 2018), I started working with **layer normalization** (https://arxiv.org/abs/1607.06450) in order to make my speech-recognition baseline model converge faster in term of number of steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problematic: \n",
    "\n",
    "Machine learning models on Speech-recogntion tasks require a lot of data to be trained. Thus, many days are required to train and finetune the model.\n",
    "In order to overcome this issue, I proposed to apply layer-normalization technique to my model. In next section, I will explain how this technique works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization explained : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand layer normalization, recall that a minibatch consists of multiple examples with the same number of features. \n",
    "The mini-batches are tensors where the **first axis** correspond to the **batches** and the other/others axis correspond to the features. The key feature of layer-normalization is that it normalizes over the axis of the features and not of the batches as the batch normalization normally does.\n",
    "\n",
    "![Layer Normalization explained](images/LN-explained.png \"\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " My baseline model consists of an encoder/decoder and attention-mechanisms in between. The encoder is a pyramidal-bidirectional-LSTM ( Listener ) and the decoder is an attention-based recurrent network decoder (RNN) that emits characters as outputs. \n",
    "\n",
    "A quick reminder that the basic LSTM equations used for these experiments are given by: \n",
    " \n",
    " ![LSTM equations](images/lstm-equations.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer normalization in our case will be applied as following: \n",
    "\n",
    "![LSTM equations after applying layer noramlization](images/lstm-ln-equations.png \"LSTM-LN-equations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow already did implement the Layer-Normalization (see: [tf.contrib.rnn.LayerNormBasicLSTMCell]( https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell ) ) and I used their implementation in first place. \n",
    "I applied layer normalization in both encoder and the decoder.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the validation loss of this experiment: the blue curve corresponds to the model with Layer Normalization applied to the encoder/decoder and the orange plot corresponds to the model without LN. \n",
    "Both experiments were applied to the 100-clean-data set of the Librispeech.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Layer normalization applied to both en/de](images/validation_en_de_layer_norm_basic.png \"L\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did another experiment, where I applied layer normalization **only to the encoder** and here is the result:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Layer Normalization applied only to the encoder](images/validation_en_layer_norm_basic.png \"Layer Normalization applied only to the encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did another experiment, where I applied layer normalization **only to the decoder** and here is the result:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Layer normalization applied only to the decoder](images/only_decoder_BLN.png \"Layer normalization applied only to the decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the LayerNormBasicLSMTCell failed to beat the BasicLSMTCell. <br>\n",
    "To investigate why this is the case, I tried to solve small tasks using the LayerNormBasicLSTMCell to inspect its behaviour.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST task using the tf.contrib.LayerNormBasicLSTMCell\n",
    "\n",
    "I started by working on the MNIST task. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Layer normalization applied to MNIST task]( images/mnist-ln.png \"Layer normalization applied to MNIST task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird reuslts, the loss value was equal **NAN** during all steps and the training accuracy is too small. <br>\n",
    "The training ended with a **testing accuracy** of 0.078125.\n",
    "\n",
    "The same model was trained with simple BasicLstmCell and it gave good results and ended-up with a **testing accuracy** of 0.8828125. <br> \n",
    "It is not the best result you can get on MNIST task but way more better than the performance using the LayerNormBasicLSTMCell. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Recurrent Attention Writer task using the tf.contrib.LayerNormBasicLSTMCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The draw task was mentionned by the autors of the LayerNormalization paper (https://arxiv.org/pdf/1607.06450.pdf).<br>\n",
    "I tried to reproduce the results the paper has using the tf.contrib.rnn.BasicLSTMCell and here are the results I got:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Layer normalization applied to DRAW task]( images/draw_ln.png \"Layer normalization applied to DRAW task\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and here the plot of the draw task **without layer normalization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DRAW task without layer Normalization]( images/draw_simple.png \"DRAW task without layer Normalization\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly the LayerNormalization task gave the same performance as the Baseline model. Except that it was more stable (less perturbations ) between the 15 and 100 epochs. <br>\n",
    "**Question:** Why was the model here more stable and converges and our LAS model didn't? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, let's go back to the original paper of the DRAW task and see how the model is designed (see \n",
    " https://arxiv.org/pdf/1502.04623v1.pdf ). <br>\n",
    "The DRAW model is composed of an encoder-decoder (both RNN ) and in between there is an attention mechanism. Close design to our LAS model, except that the LAS model has a more complex encoder architecture (Pyramidal bidirectional LSTM) and of course longer and more complex sequences ( audio data vs MNIST images ).\n",
    "\n",
    "Here is the architecture of the DRAW model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DRAW model architecture](images/Draw.png \"DRAW model architecture\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propsed solutions : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having in mind all those information, I came back and modified the LAS model where I set up a more simple listener architecture: instead of using a Pyramidal-Bidirectional-LSTM, why not just using a simple Bidirectional-LSTM and see if I can make a difference this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified LAS : ( Encoder as a Bidirectional LSTM with layer normalization )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, even with a simpler speller I was not able to have satisfying results.<br>\n",
    "But this time the model was able to converge more and the validation loss reached 1.10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoder as a simple Bidirectional LSTM](images/bidirectional_ln.png \"Encoder as a simple Bidirectional LSTM\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified LAS: ( modified Encoder wher layer normalization applied only to the Pyramidal Layers ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this solution came with non-satisfing results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Layer normalization applied only to the Pyramidal layers of the encoder](images/pyramidal_ln.png \"Layer normalization applied only to the Pyramidal layers of the encoder\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step and after all those experiments, I took the decision to give up working with the tf.contrib.rnn.LayerNormBasicLSTMCell and to implement my own code for layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my implementation of the Layer-Normalization inspired by https://github.com/hardmaru/supercell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d073179/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Module for constructing RNN Cells.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.compiler import jit\n",
    "from tensorflow.contrib.layers.python.layers import layers\n",
    "from tensorflow.contrib.rnn.python.ops import core_rnn_cell\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import op_def_registry\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import gen_array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_impl  # pylint: disable=unused-import\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import partitioned_variables  # pylint: disable=unused-import\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import rnn_cell_impl\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.contrib.layers.python.layers import layers\n",
    "\n",
    "\n",
    "class LN_LSTMCell(rnn_cell_impl.RNNCell):\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tLSTM unit with layer normalization \n",
    "\t\tLayer normalization implementation is based on:\n",
    "\t\thttps://arxiv.org/abs/1607.06450.\n",
    "\t\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tdef __init__(\n",
    "\t\t\t\tself,\n",
    "\t\t\t\tnum_units,\n",
    "\t\t\t\tforget_bias=1.0,\n",
    "\t\t\t\tinput_size=None,\n",
    "\t\t\t\treuse=None,\n",
    "\t\t\t\tuse_layer_norm=True,\n",
    "\t\t\t\tuse_recurrent_dropout=False, dropout_keep_prob=0.90,\n",
    "\t\t\t\ttraining= True,\n",
    "\t\t\t\t):\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tInitializes the basic LSTM cell.\n",
    "\t\t\t\tArgs:\n",
    "\t\t\t\t\tnum_units: int, The number of units in the LSTM cell.\n",
    "\t\t\t\t\tforget_bias: float, The bias added to forget gates (see above).\n",
    "\t\t\t\t\tinput_size: Deprecated and unused.\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\"\"\"\n",
    "\n",
    "\t\t\t\tsuper(LN_LSTMCell, self).__init__(_reuse=reuse)\n",
    "\n",
    "\t\t\t\tif input_size is not None:\n",
    "\t\t\t\t\t\tlogging.warn('%s: The input_size parameter is deprecated.',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t self)\n",
    "\n",
    "\t\t\t\tself._num_units = num_units\n",
    "\t\t\t\tself._forget_bias = forget_bias\n",
    "\t\t\t\tself._reuse = reuse\n",
    "\t\t\t\tself.use_recurrent_dropout = use_recurrent_dropout\n",
    "\t\t\t\tself.dropout_keep_prob = dropout_keep_prob\n",
    "\t\t\t\tself.use_layer_norm = use_layer_norm\n",
    "\n",
    "\t\t@property\n",
    "\t\tdef state_size(self):\n",
    "\t\t\t\treturn rnn_cell_impl.LSTMStateTuple(self._num_units,\n",
    "\t\t\t\t\t\t\t\tself._num_units)\n",
    "\n",
    "\t\t@property\n",
    "\t\tdef output_size(self):\n",
    "\t\t\t\treturn self._num_units\n",
    "\n",
    "\t\tdef call(self, x, state):\n",
    "\t\t\twith tf.variable_scope(\"ln\"):\n",
    "\t\t\t\th, c = state\n",
    "\n",
    "\t\t\t\th_size = self._num_units\n",
    "\t  \n",
    "\t\t\t\tbatch_size = x.get_shape().as_list()[0]\n",
    "\t\t\t\tx_size = x.get_shape().as_list()[1]\n",
    "\t\t\t\t  \n",
    "\t\t\t\tw_init= None # uniform\n",
    "\n",
    "\t\t\t\th_init=lstm_ortho_initializer()\n",
    "\n",
    "\t\t\t\tW_xh = tf.get_variable('W_xh',\n",
    "\t\t\t\t\t[x_size, 4 * self._num_units], initializer=w_init)\n",
    "\n",
    "\t\t\t\tW_hh = tf.get_variable('W_hh_i',\n",
    "\t\t\t\t\t[self._num_units, 4*self._num_units], initializer=h_init)\n",
    "\n",
    "\t\t\t\tbias = tf.get_variable('bias',\n",
    "\t\t\t\t\t[4 * self._num_units], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\t\txh = tf.matmul(x,W_xh)\n",
    "\t\t\t\thh = tf.matmul(h,W_hh)\n",
    "\t\t\t\tln_xh = raw_layer_norm(xh)\n",
    "\t\t\t\tln_hh = raw_layer_norm(hh)\n",
    "\t\t\t\tconcat = ln_xh + ln_hh + bias \n",
    "\t\t\t\t#concat = xh + hh + bias\n",
    "\t\t\t\ti, j, f, o = tf.split(concat, 4, 1)\n",
    "\t\t\t\tg = tf.tanh(j) \n",
    "\t\t\t\tnew_c = c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i)*g\n",
    "\t\t\t\tnew_h = tf.tanh(layer_norm(new_c,self._num_units,scope='ln_c')) * tf.sigmoid(o)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn new_h, tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
    "\t\t\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\t\n",
    "def lstm_ortho_initializer(scale=1.0):\n",
    "  def _initializer(shape, dtype=tf.float32, partition_info=None):\n",
    "\tsize_x = shape[0]\n",
    "\tsize_h = shape[1]//4 # assumes lstm.\n",
    "\tt = np.zeros(shape)\n",
    "\tt[:, :size_h] = orthogonal([size_x, size_h])*scale\n",
    "\tt[:, size_h:size_h*2] = orthogonal([size_x, size_h])*scale\n",
    "\tt[:, size_h*2:size_h*3] = orthogonal([size_x, size_h])*scale\n",
    "\tt[:, size_h*3:] = orthogonal([size_x, size_h])*scale\n",
    "\treturn tf.constant(t, dtype)\n",
    "  return _initializer\n",
    "\n",
    "def orthogonal(shape):\n",
    "\tflat_shape = (shape[0], np.prod(shape[1:]))\n",
    "\ta = np.random.normal(0.0, 1.0, flat_shape)\n",
    "\tu, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "\tq = u if u.shape == flat_shape else v\n",
    "\treturn q.reshape(shape)\n",
    "\n",
    "\n",
    "def orthogonal_initializer(scale=1.0):\n",
    "\tdef _initializer(shape, dtype=tf.float32, partition_info=None):\n",
    "\t\treturn tf.constant(orthogonal(shape) * scale, dtype)\n",
    "\treturn _initializer\n",
    "\n",
    "\n",
    "def layer_norm(x, num_units, scope=\"layer_norm\", reuse=False, gamma_start=1.0, epsilon = 1e-5, use_bias=True):\n",
    "  axes = [1]\n",
    "  mean = tf.reduce_mean(x, axes, keep_dims=True)\n",
    "  x_shifted = x-mean\n",
    "  var = tf.reduce_mean(tf.square(x_shifted), axes, keep_dims=True)\n",
    "  inv_std = tf.rsqrt(var + epsilon)\n",
    "  with tf.variable_scope(scope):\n",
    "    if reuse == True:\n",
    "      tf.get_variable_scope().reuse_variables()\n",
    "    gamma = tf.get_variable('ln_gamma', [num_units], initializer=tf.constant_initializer(gamma_start))\n",
    "    if use_bias:\n",
    "      beta = tf.get_variable('ln_beta', [num_units], initializer=tf.constant_initializer(0.0))\n",
    "  output = gamma*(x_shifted)*inv_std\n",
    "  if use_bias:\n",
    "    output = output + beta\n",
    "  return output\n",
    "\n",
    "def raw_layer_norm(x, epsilon=1e-3):\n",
    "\t  axes = [1]\n",
    "\t  mean = tf.reduce_mean(x, axes, keep_dims=True)\n",
    "\t  std = tf.sqrt(\n",
    "\t      tf.reduce_mean(tf.square(x - mean), axes, keep_dims=True) + epsilon)\n",
    "\t  output = (x - mean) / (std)\n",
    "\t  return output\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now the results I have with the custom implementation of the Layer normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first attempt, I applied my custom code to the MNIST task and surprisingly I got the following results: <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Custom layer normalization applied to MNIST task](images/mnist_custom.png \"LSTM-LN-equations\")  <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment ended with a **testing Accuracy of 0.9921875** way better than the one I got with tf.contrib.rnn.LayerNormBasicLSTMCell 0.078125 .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAS with my custom layer normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied in this experiment my custom layer normalization on both encoder,decoder and here the result I got: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Custom layer Normalization applied to both encoder and decoder](images/Custom_ln.png \"Custom layer Normalization applied to both encoder and decoder\")  <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite that the results were not as expected. We can see the big dropout in the validation loss the LAS model was made in fewer steps. <br>\n",
    "This result is considered as the best one I got but still I can't figure out why the model stops converging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, I can say that it looks like I'm getting into the black magic of tweaking my model. \n",
    "I'm not sure if it would be enough to tune the hyperparameters. <br>\n",
    "Another important issue I detected was that the MFCC features were all normalied during the pre-processing phase.\n",
    "That could be the case why the layerNormalization didn't work. As if the model is trying to normalize an already normalized distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio features processor: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting my audio processor, I saw that all my features were normalized. This can be an explication of why the layer normalization didn't work on my model on previous experiments. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean and variance normalize the features\n",
    "        if self.conf['mvn'] == 'True':\n",
    "            features = (features-np.mean(features, 0))/np.std(features, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "1) re-generate the features without normalizing them and apply after that layer normalization to my model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
